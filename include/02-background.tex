%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Background
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Theoretical background}
\label{sec:background}

\section{Fluid-Structure Interaction}
Fluid-Structure Interaction (FSI) studies the interaction between a structure (solid) and a fluid flow (liquid or gas) around it. It is a multi-physics problem which has large interest in diversified fields such as mechanical engineering (e.g. airfoils), civil engineering (e.g. towers) or medicine technique (e.g. artificial heart valves). Based on the response of structure and fluid fields, it is classified as one-way or two-way fluid-structure interaction problem. If the structural displacement/deformation does not influence the flow fluid or vice versa, then the FSI system is termed to be one-way fluid-structure interaction system. Conversely if the fluid flow and the displacement or deformation of the structure have significant influence on each other then the FSI system is termed as two-way fluid-structure interaction problem. Figure \ref{fig:2.1} represents a typical domain of a fluid-structure interaction problem, $\Omega$ refers to the common domain, $ \Omega_{f} $ the fluid domain and $ \Omega_{s} $ the structure domain. Figure   \ref{fig:2.2a} and \ref{fig:2.2b} represents one-way and two-way FSI fluid-structure interaction problems respectively.\\

\begin{figure}[H]
	\centering
	\includegraphics[height=3in]{fsi_domain}
	\caption{Representational figure of Fluid-Structure Interaction domain,taken from \citet{Richter}}
	\label{fig:2.1}
\end{figure}

\begin{figure}[h]
  \centering
  \captionsetup{justification=centering}
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=\linewidth]{one-way-fsi}
    \caption{Structure domain influencing the fluid domain}
    \label{fig:2.2a}
  \end{subfigure}
  \begin{subfigure}[b]{0.8\linewidth}
    \includegraphics[width=\linewidth]{two-way-fsi}
    \caption{Both structure and fluid domains influencing each other}
    \label{fig:2.2b}
  \end{subfigure}
  \caption{Representational figure of one-way \ref{fig:2.2a}, and two-way \ref{fig:2.2b} fluid-structure interaction domains,taken from \citet{Richter}}
  \label{fig:2.2}
\end{figure}

\subsection{Numerical simulation strategies of FSI problems}
Numerical simulation of FSI involves solving set of differential equations and corresponding boundary conditions for fluid and structural fields respectively. Suitable interface conditions needs to be defined so as the structural and fluid domains are well distinguished. There are different methodologies implemented to solve the FSI problem and are well documented in the literature. An overview of the numerical solution procedure for FSI problems are presented below.\\
The FSI problem is classified based on solution approaches and on the treatment of mesh handling techniques as represented below. A brief overview of these methods are presented subsequently.

\begin{itemize}
 \item{Classification based on numerical solution approaches}
 \begin{itemize}
 \item{Monolithic solver}
 \item{Partitioned solver}
 \begin{itemize}
 \item{Explicit coupling}
 \item{Implicit coupling}
 \end{itemize}
 \end{itemize}
 \item{Classification based on meshing strategies}
 \begin{itemize}
 \item{Conforming mesh}
 \item{Non-conforming mesh}
 \end{itemize}
\end{itemize} 

\subsubsection{Classification based on numerical solution approaches}
This classification is based on how the fluid and structural fields are getting solved. In \textit{monolithic solvers} both the fluid and structural domains are expressed by single set of equations, whereas in \textit{partitioned solver approach} structural fields and fluid fields are solved separately and additionally requires coupling of the distinct solvers. A brief introduction to these methods are represented below.    

 \subsubsection*{Monolithic solver approach}
The equations governing the fluid flow and the displacement/deformation of the structure are represented by single set of equations which are then solved simultaneously by an unified algorithm, within a single solver framework \citet{{Richter},{Becker_2014},{hubner2004monolithic},{michler2004monolithic}}. The central idea of monolithic solvers is to represent the interface by an homogeneous discretization, thus maintaining the conservation properties at the interface \citet{michler2004monolithic} \citet{van2003energy}. Although monolithic approach gives strong coupling between the fields, it is commonly considered to be impractical for real world applications. It also demands enormous computational power for solving such a large system of equations since it has to incorporate the behavior of both fluid flow and solid structure. Figure \ref{fig:2.3} represents a schematic of the monolithic and partitioned solver approaches in solving FSI problems.
    	  
\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{monolithic}
  \caption{Schematic representation of Monolithic and Partitioned solution approaches. $ S^{f} $ represents the fluid solver and $ S^{s} $ represents the structural solver for two successive time steps current $ (t_{n}) $  and next $(t_{n+1})$. Figure taken from \citet{hou2012numerical}}
  \label{fig:2.3}
\end{figure}

 \subsubsection*{Partitioned solver approach}
Another approach to fluid-structure interaction is to use two distinct solvers to model both fluid and solid domains. This technique allows the coupling of the fluid and solid solution by maintaining suitable coupling conditions. The interface conditions are used explicitly to communicate information between the fluid and structure solutions.\\

Some coupling algorithms were suggested by various studies (\citet{{piperno1995partitioned},{felippa2001partitioned},{farhat2000two}}) which allows for reuse of existing codes that have been developed for each field. This approach is very robust and can be used for wide variety of applications. The major disadvantage of this approach is that, the interface location that divides the fluid and the structure domains is not known a-priori and usually changes in time. Thus, the partitioned approach requires tracking of the new interface location and its related quantities, which can be cumbersome and error-prone. The interface coupling conditions that are as stated below, have to be satisfied in order to have a stable solution.

\begin{itemize}
\item Kinematic coupling condition: The displacements, velocities and accelerations of the sub-zones have to be equal at the interface at any point in time.

$\psi^{CFD}_{\Gamma}(t) = \psi^{CSD}_{\Gamma}(t)$, $\dot{\psi}^{CFD}_{\Gamma}(t) = \dot{\psi}^{CSD}_{\Gamma}(t)$, $ \ddot{\psi}^{CFD}_{\Gamma}(t) = \ddot{\psi}^{CSD}_{\Gamma}(t) $

\item Dynamic coupling condition: Conservation of the dynamic equilibrium of all forces at the interface needs to be satisfied. (Action and reaction forces must cancel out each other)

$ f^{CFD}_{\Gamma}(t) = -f^{CSD}_{\Gamma}(t) $
\end{itemize}

Depending on the influence of the structure movement on the fluid field the FSI problems are further subdivided into two following categories:

\begin{itemize}
 \item Explicit/Weakly coupled:	In an explicitly coupled algorithm, the equations of fluid mechanics,
structural mechanics and the relative mesh movement are solved sequentially. Initially the governing equation of fluid is solved with the velocity boundary condition derived from the structural displacement. The structural mechanics equations are solved next with the forces obtained from the fluid solver. Finally the grid is adapted based on the structural displacement. This coupling scheme is termed as weak coupling, as there is no sub iteration between the two sub zones.

 \item Implicit / Strongly coupled: In a strongly/implicitly coupled system, sub iterations between both the fluid and structural solvers are carried out at every time step till convergence is achieved.  The convergence is determined by maintaining a small enough structural residuum. Detailed explanation on this, is presented in the subsequent section. This approach is implicit in nature and more robust, however requires additional computation time.
\end{itemize}

Figure \ref{fig:2.4} represents the schematic representation of explicit and implicitly coupled, partitioned approach to solving fluid structure interaction. In the presented study \textit{semi-implicit predictor-corrector} coupling scheme is used. 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=2in]{explicit_coupling}
    \caption{Explicit coupling}
    \label{fig:2.4a}  
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=2in]{implicit_coupling}
    \caption{Implicit coupling}
    \label{fig:2.4b} 
  \end{subfigure}
  \caption{Representational sketch of explicit coupling \ref{fig:2.4a}, and implicit coupling \ref{fig:2.4b} approaches to solving partitioned, fluid-structure interaction problems,taken from \citet{munsch2015entwicklung}}
  \label{fig:2.4}
\end{figure}

\subsection{Classification based on meshing strategies}\par
The FSI problem is further classified based on the meshing strategies implemented to couple the fluid and structure domains. They are classified into two methods:\textit{conforming mesh} and \textit{non-conforming mesh}\\

\begin{itemize}
 \item Conforming mesh method: In conforming mesh methods, the interface governing the fluid and structural domains is resolved such that the node connectivity is maintained between the domains. In this method, the interface conditions is considered as physical boundary conditions, which treat the interface location as part of the solution. Owing to the movement and/or deformation of the solid structure,re-meshing (or mesh-update) is required.
 
 \item Non-conforming mesh method: The non-conforming mesh methods treat the interface location as constraints imposed on the model equations so that non-conforming meshes (node-connectivity need not be maintained between the domains) can be employed. As a result, the fluid and solid equations can be conveniently solved independently from each other with their respective grids, and re-meshing is not necessary. The distinction between these two types of meshes can be observed in figure \ref{fig:2.5}, where a solid body (a sphere) is moving in a fluid domain.  
 \end{itemize} 
 
\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=2.25in]{conforming_mesh}
    \caption{Conforming mesh method representation at time $ t_{n} $ and $t_{n+1}$}
    \label{fig:2.5a} 
  \end{subfigure}
  \begin{subfigure}[b]{0.45\linewidth}
    \includegraphics[width=2.25in]{non-conforming_mesh}
    \caption{Non-conforming mesh method representation at time $ t_{n} $ and $t_{n+1}$}
    \label{fig:2.5b}  
  \end{subfigure}
  \caption{Examples of Conforming mesh \ref{fig:2.5a} and Non-conforming mesh \ref{fig:2.5b} methods, taken from \citet{hou2012numerical}.}
  \label{fig:2.5}
\end{figure} 

\section{Governing equations of fluid flow and structural deformations}

In Fluid-Structure Interaction problems, the solid moves through the fluid domain due to different forces or excitation. The computation domain changes with time and must be considered during the simulation. An approach which aims to solve this particular problem is Arbitrary Eulerian Lagrangian formulation, popularly referred to ALE formulation. The governing equation for conservation of mass and momentum are reformulated for a moving grid which are time dependent control volume and surface integrals. The governing equations for a Newtonian incompressible fluid in ALE formulation is given as follows.

\begin{align}
%  {\frac{d}{dt}\int_{V(t)}\rho dV+\int_{S(t)}\rho(U-U_{g}).\vec{n} dS}=0 
 \frac{\partial{u_{j}}}{\partial{x_{i}}} & = 0\\
 \frac{\partial{u_{i}}}{\partial{t}}+\left(u_{i}-v_{g,i}\right)\frac{\partial{u_{i}}}{\partial{x_{j}}} &= \frac{1}{\rho}\frac{\partial{p}}{\partial{x_{i}}}+\nu\frac{\partial^2{u_{i}}}{\partial{x_{j}}\partial{x_{j}}}-\frac{\partial{\tau_{ij}}}{\partial{x_{j}}}\\
 \frac{1}{J}\frac{\partial{J}}{\partial{t}}-\frac{\partial{v_{g,i}}}{\partial{x_{i}}} & = 0
\end{align}\\
The equations presented are \textit{conservation of mass},\textit{conservation of momentum} and \textit{Space Conservation Law} (proposed by \citet{demirdvzic1988space}) in ALE formulation in order to account for the moving grid representation. Here $ v_{g} $ is the grid cell velocity and $J$ is the determinant of the metric tensor. Numerical treatment of these equations are presented in {refer to the relevant section}.

\subsection{Coupling of fluid and structural solvers}

The FSI problem in this study is solved by a \textit{semi-implicit predictor corrector} coupling scheme, which is a strong coupling method between fluid and structural solver. Details about this scheme is explained in \citet{breuer2012fluid}, summary of this scheme is presented in brief as follows:
\begin{enumerate}[(a)]
 \item At the start of the time step, the displacement of the structure is predicted based on values from the previous time step.
 \item A predictor corrector approach is implemented to solve for conservation of momentum and mass (velocity $u_{i}$ and pressure $p$). This is explained in detail in section {refer to relevant section}
 \item The calculated forces are then tranferred to the Computational Structural Dynamics (CSD) solver. Generalized-$\alpha$ method is used to solve the conservation equations of the structural solver. This is explained in detail in section {refer to relevant section}.
 \item The FSI-subiterations are performed until desired structural and fluid convergence is achieved for a particular time step. Dynamic calculation of the under-relaxation factor is used to enhance the convergence{refer to relevant section}. The corrector step takes the predicted velocity $\overset{\star}{u}_{i}$ as approximation to obtain the new corrected velocity $u_{i}$ and pressure $p$. 
 \item Mesh adaptation for each FSI-subiteration is performed, based on transfinite interpolation by \citet{thompson1985numerical}. A comprehensive implementation can be found in \citet{munsch2011numerical}. 
\end{enumerate}
\par
The predcitor-corrector algorithm for a strongly coupled, partitioned FSI solver is represented in the flowchart \ref{fig:2.6}

\begin{figure}[h]
 \includegraphics[height=3.5in]{p-c}
 \caption{Flowchart representing the predictor-corrector coupling scheme to solve a strongly coupled, partitioned FSI solver \citet{munsch2011numerical}}
 \label{fig:2.6}
\end{figure}

\subsubsection*{Convergence of the FSI sub-iterations}
As explained in flowchart \ref{fig:2.6} for an implicitly coupled, partitioned FSI solver, the flow field is determined in the actual flow geometry. From this, the friction and pressure forces on the interacting walls are computed. These are boundary conditions to the structural solver. The structural solver computes the deformations, with which the fluid mesh is then modified. Afterwards the flow solver is started again. 

The FSI sub-iteration loop is repeated until a convergence criteria $\epsilon$ is satisfied, which is determined by the variation of the mean displacements \ref{eqn:2.4}.

\begin{equation}\label{eqn:2.4}
\frac{1}{N}\sum_{n=1}^{k}\frac{\norm{\mathbf{d}_{i}^{n+1}-\mathbf{d}_{i+1}^{n+1}}^2}{\norm{\mathbf{d}_{i+1}^{n+1}-\mathbf{d}_{i+1}^{n}}^2}<\epsilon
\end{equation}

In order to accelerate the rate of convergence, a relaxation factor is added to the displacement and force values. The resulting displacement is computed from the current sub-iteration displacement value and displacement value from the last sub-iteration. Under-relaxation of the displacements are done as given by the equation \ref{eqn:2.5}, where $\mathbf{\omega}$ represents the under-relaxation factor.

\begin{equation}\label{eqn:2.5}
\mathbf{d}_{i+1}^{n+1}=\mathbf{d}_{i}^{n+1}+\mathbf{\omega_{i}}\left(\tilde{\mathbf{d}}_{i+1}^{n+1}-\mathbf{d}_{i}^{n+1}\right)
\end{equation} 

Relaxation of the interface displacements is synonymous to the line search step of the non-linear solver. There are different methods to calculate the under-relaxation parameter. The most simplest method is by using a \textit{constant under-relaxation parameter} for all time-steps. There are various methods available now to determine the under-relaxation parameter dynamically, however the presented study focuses on \textit{Aitken relaxation method} and relaxation via the method of \textit{steepest descent}. An introduction to these methods is presented below.

\begin{enumerate}[(i)]
 \item Constant under-relaxation method: The most simplest and ineffective method is to choose a constant $\mathbf{\omega}$ for all time-steps. The relaxation parameter has to be small enough to keep the iteration from diverging, but as large as possible in order to use as much of the new solution as possible and to avoid unnecessary FSI iterations. The optimal $\mathbf{\omega}$ value is problem specific and not known a priori. Furthermore even the optimal fixed value could lead to more iterations than a suitable dynamic relaxation parameter.
 \item Aitken relaxation method: It is an effective and cheap method to calculate the under-relaxation parameter dynamically. The central idea of Aitken's $\Delta^2$ method is to use values from two previous iterations to improve the current solution, proposed by \citet{irons1969version}. The aitken factor $\mathbf{\mu}$ \ref{eqn:2.6} is calculated from the displacement values, and the under-relaxation factor $\mathbf{\omega}$ is calculated as given in the equation \ref{eqn:2.9} \citet{mok2001partitioned}. This method has been implemented and validated with many FSI applications \citet{{kuttler2008fixed},{irons1969version}} .(Refer some more papers which uses aitken) The relaxation parameter calculation is valid in every FSI sub-iteration step. This method requires values from two previous sub-iterations, thus the relaxation parameter can be calculated after the first FSI sub-iteration. In the presented study the aitken factor from last time-step is used as an initial value for the current time-step $\left(\mathbf{\mu}_{i_{max}}^{n} = \mathbf{\mu}_{1}^{n+1}\right)$ as proposed by \citet{irons1969version}.      
\begin{align}\label{eqn:2.6}
\mu_{i}^{n+1} = \mu_{i-1}^{n+1}+\left(\mu_{i-1}^{n+1}-1\right)\frac{\left(\Delta\mathbf{d}_{i}^{n+1}-\Delta\mathbf{d}_{i+1}^{n+1}\right)^{T}\Delta\mathbf{d}_{i+1}^{n+1}}{\left(\Delta\mathbf{d}_{i}^{n+1}-\Delta\mathbf{d}_{i+1}^{n+1}\right)^{T}\left(\Delta\mathbf{d}_{i}^{n+1}-\Delta\mathbf{d}_{i+1}^{n+1}\right)}\\
\mathbf{d}_{i}^{n+1} = \mathbf{d}_{i-1}^{n+1}-\mathbf{d}_{i}^{n+1}\\
\mathbf{d}_{i+1}^{n+1} = \mathbf{d}_{i}^{n+1}-\tilde{\mathbf{d}}_{i+1}^{n+1}
\end{align}  
\begin{equation}\label{eqn:2.9}
\mathbf{\omega}_{i} = 1-\mu_{i}^{n+1}
\end{equation}
 \item Steepest-Descent method: The best relaxation parameter possible is the one that finds the optimal step length in $r_{i+1}^{n+1}$ direction. In order to obtain that, the existence of a merit function $\varphi$ is assumed, that is minimal at the solution $d_{i+1}^{n+1}$ and sufficiently smooth, such that the relaxation parameter $\mathbf{\omega}_{i}$ is given by equation \ref{eqn:2.10}. By Taylor's series expansion and assumption of connection between the merit function and interface residual, the final expression for the relaxation parameter is given by the equation \ref{eqn:2.11}. The steps in obtaining this expression is explained clearly in \citet{kuttler2008fixed}.
 
 \begin{equation}\label{eqn:2.10}
  \mathbf{\omega_{i}} = arg \min_{\mathbf{\omega}_{i}} \varphi(d_{i}^{n+1}+\omega_{i}r_{i+1}^{n+1})
 \end{equation}
 
 \begin{equation}\label{eqn:2.11}
  \mathbf{\omega_{i}} = -\frac{\left(\mathbf{r}_{i+1}^{n+1}\right)^T\left(\mathbf{r}_{i+1}^{n+1}\right)}{\left(\mathbf{r}_{i+1}^{n+1}\right)^T\mathbf{J}\left(\mathbf{r}_{i+1}^{n+1}\right)} 
 \end{equation}\\
  In the equation \ref{eqn:2.11},$\mathbf{J}$ represents the Jacobian matrix of the interface residual $\left(\mathbf{J} = \varphi^{"}(\mathbf{d}_{i}^{n+1})\right)$. This interface Jacobian is not readily available, there are methods to obtain the matrix vector product which can be performed with black-box solvers \citet{kuttler2008fixed}. In the presented study, the matrix vector product is obtained by means of Finite-Difference method as expressed in equation \ref{eqn:2.12}.
 
 \begin{align}\label{eqn:2.12}
   \mathbf{J}\mathbf{r}_{i+1}^{n+1} &= \frac{\mathbf{fsi_{sub}}\left(\mathbf{d}_{i}^{n+1}+\delta\mathbf{r}_{i+1}^{n+1}\right)-\mathbf{d}_{i}^{n+1}-\delta\mathbf{r}_{i+1}^{n+1}-\mathbf{r}_{i+1}^{n+1}}{\delta}\\
   \delta &= \lambda\left(\lambda+\frac{\abs{\mathbf{d}_{i}^{n+1}}}{\abs{\mathbf{r}_{i+1}^{n+1}}}\right) 
 \end{align}\\
 A small $\lambda$ is assumed in the study, this method requires one more fsi-sub iteration cycle in order to evaluate the interface Jacobian. The computational costs for this method is slightly higher. 
 \end{enumerate} 
 
\section{Flow separation and vortex shedding}
The presence of the fluid viscosity slows down the fluid particles very close to the solid surface and forms a thin slow-moving fluid layer called a boundary layer. The flow velocity is zero at the solid surface to satisfy the no-slip boundary condition. Inside the boundary layer, flow momentum is quite low since it experiences a strong viscous flow resistance. Therefore, the boundary layer flow is sensitive to the external pressure gradient (as the form of a pressure force acting upon fluid particles). If the pressure decreases in the direction of the flow, the pressure gradient is said to be favorable. In this case, the pressure force can assist the fluid movement and there is no flow retardation. However, if the pressure is increasing in the direction of the flow, an adverse pressure gradient condition exists. In addition to the presence of a strong viscous force, the fluid particles now have to move against the increasing pressure force. Therefore, the fluid particles could be stopped or reversed, causing the neighboring particles to move away from the surface. This phenomenon is called the boundary layer separation or flow separation.\\
In fig.\ref{fig:2.7} there exists a pressure gradient $\frac{\partial{p}}{\partial{x}}<0$ to accelerate the fluid along the surface $x$ from D to E and a gradient $\frac{\partial{p}}{\partial{x}}>0$ to decelerate the fluid between E and F. Potential energy (PE) in the pressure field is converted to kinetic energy between D and E, then fully back to PE between E and F, because there is no source of dissipation in an inviscid fluid.\\
 
\begin{figure}[h]
 \centering
 \includegraphics[height=3.5in]{pressure_fpac}
 \caption{Pressure field and separation point for flow past a cylinder.}
 \label{fig:2.7}
\end{figure}

In the inviscid region, the fluid momentum is sufficient to overcome the adverse pressure gradient barrier and reach point F with the same kinetic energy as at point D. However, inside the boundary layer (near the surface of the cylinder) energy is dissipated by the viscous drag. As the momentum is smaller near the wall than in the regions closer to the free stream, a fluid particle on the boundary layer can not overcome the pressure barrier and comes to a standstill and eventually gets pushed backwards into motion by the pressure distribution of the outer flow. This continuous retardation of the flow brings the wall shear stress $\tau_{w}$ to zero at the point S on the wall as shown in figure \ref{fig:2.8} . The shear stress starts to become negative after the point S, which is the separation point. The boundary layer separation point is given by the mathematical relationship in equation \ref{eqn:2.13}.\\
\begin{equation}\label{eqn:2.13}
 \tau_{w} = \mu\left(\frac{\partial{u}}{\partial{y}}\right)
\end{equation}
\begin{figure}[h]
 \centering
 \includegraphics[height=3in]{advpressure}
 \caption{point of separation and Wall Shear stress,ref \citet{schlichting1955boundary}}
 \label{fig:2.8}
\end{figure}
Due to the existence of the high shear layer near the cylinder surface, the breaking mechanism is caused by Kelvin-Helmholtz instability. This instability leads to a rolling up and to a separation from the surface of the cylinder that result in the formation of the vortex street (\citet{{lin1996low},{nishioka1978mechanism},{posdziech2007systematic}}). This breaking mechanism is the basic physics concept of vortex formation and the near wake; that can be an origin of technique for triggering vortex. Vortex shedding occurs alternatively from top and bottom section of the cylinder. Vortex formation and separation for different Reynolds number (Re) is illustrated in figure \ref{fig:2.9}.

\begin{figure}[h]
  \centering
  \captionsetup{justification=centering}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=2in]{vortex_shedding}
    \caption{Vortex shedding influenced by Reynolds number}
    \label{fig:2.9a}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=2in]{vortex}
    \caption{Vortex formation}
    \label{fig:2.9b}
  \end{subfigure}
  \caption{Vortex formation and shedding phenomenon, ref \citet{laroussivortex}}
  \label{fig:2.9}
\end{figure}

\section{Vortex Induced Vibrations}
The periodic vortex shedding from a bluff body in a flow can cause the structure to vibrate. The vibrations alter the vortex pattern and change the spacing between vortices in the wake. These types of vibrations is called as \textit{Vortex Induced Vibrations}. The case of an elastically-mounted circular cylinder, is one of the most conceptually basic cases of this phenomenon. The cylinder acts as a rigid body, translating through the fluid, while its elastic supports deform to accommodate the motion. The studies of \citet{{feng1968measurement},{griffin1980vortex}}, covered a wide range of \textit{Reynolds number} and show that the vortex-induced vibration is a self-limiting process. The cross-flow vibration amplitude has a strong relationship with the phase difference between the lift force and the cylinder motion. According to the study of \citet{williamson19912}, the upper limit of Reynolds number, where the vortex shedding from a circular cylinder is still two-dimensional and the wake is laminar, can reach 200 under carefully controlled experimental conditions.\\
In the case of a cylinder treated as an elastically mounted spring-damper-mass system, the motion of the cylinder in perpendicular axis, is described by the equation \ref{eqn:2.15}.
\begin{equation}\label{eqn:2.15}
 m\ddot{y}+d\dot{y}+ky = F_{y}
\end{equation}
In the above equation, $y$,$\dot{y}$,$\ddot{y}$ represents the displacement, velocity and acceleration components in the direction of lift force respectively. $m$,$d$ and $k$ represents the structural mass, damping co-efficient and spring stiffness constant of the system. These parameters are defined by the reduced damping parameter $S_{g} = 8\pi^2S_{t}^2M^\star$, which represents the vibration amplitude and was proposed by  \citet{griffin1992vortex}. The mass ratio $M^\star$ is a dimensionless parameter which is the ratio of structural mass to mass of the fluid. $M^{\star}=\frac{m_{s}}{\rho\mathbf{D^{2}}\mathbf{h}}$ where $m_{s}$ represents the structural mass of the cylinder. The \textit{Strouhal number} $S_{t}$ is a dimensionless number which normalizes the Vortex shedding frequency $f_{s}$ and is given by, $S_{t}=\frac{f_{s}D}{U_{\infty}}$. From the studies of \citet{zhou1999vortex}, the strouhal number of a fixed cylinder at Re=200 is found to be 0.195. The same value has been considered for this present study. The calculations of the relevant parameters is presented in the subsequent section.
\subsection{Calculation of parameters influencing the vortex induced vibrations}
\begin{enumerate}[(a)]
\item Structural Mass $m_{s}$: The structural mass $m_{s}$ is given by the general mass formula:\\
  \begin{align}
  {m_{s}} =  \rho\frac{1}{4}\pi{D^{2}}{L}
  \end{align}
  where $\rho$, L and D represents the density, length and diameter of the cylinder. In the present study,$\rho=1kg/m^{3}$, L=D=1$m$, hence $m_s=\frac{\pi}{4} kg$.
\item Spring Stiffness constant $k$: The stiffness constant of the system is defined by the equation:\\
 \begin{align}
 {k} = {m_{eff}}{\omega}_{n}^{2}\\
 \omega_{n} = 2{\pi}{f_{n}}\\
 {k} = {m_{eff}}\left(2\pi{f_{s}}\right)^2
 \end{align}
where $\omega_{n}$ represents the natural angular frequency of the system and $m_{eff}$ is the effective mass which is the sum of structural mass and fluid mass. The frequency ratio $\left(\frac{f_{n}}{f_{s}}\right)$ of $1$ is considered in this present study.
\item Damping co-efficient $d$: The damping co-efficient of the spring-mass-damper system is calculated from the reduced damping parameter $S_{g}$, which is defined by the equation \ref{eqn:2.20}.\\
 \begin{equation}\label{eqn:2.20}
 {d} = \frac{S_{g}}{8{\pi}^2{S_{t}}^2{M}^*}
 \end{equation}   
\end{enumerate}

The response of the system is studied for a Mass ratio $M^*$ of 1 and different reduced damping parameter $S_{g}$. Table \ref{table:2.1} represents the parameters calculated for different $S_{g}$, replicating the work of \citet{zhou1999vortex}.\\

%\begin{center}
 \begin{table}[H]
 \begin{centering}
  \begin{tabular}[c]{|c|c|c|c|}
 %\caption{Spring stiffness co-efficient and damping co-efficient calculated for $M^*=1$ and different $S_{g}$ values.\label{table:2.1}}
   \hline
 %\multirow{2}{*}{Parameters} & \multicolumn{3}{|c|}{M*=1}\\\cline{2-4}
   \multirow{4}{4em}{$M^{*}=1$} & $S_{g}$ & $k[N/m]$ & $d[kg/s]$\\\cline{2-4}
 %\hline
  %S_{g}=0.01 & S_{g}=0.1 & S_{g}=1\\[0.5ex]
 %\hline
 %k & 2.6802 & 2.6802 & 2.6802\\
   & 0.01 & 2.6802 & 0.0033 \\ \cline{2-4}
   & 0.1 & 2.6802 & 0.0333 \\ \cline{2-4}
   & 1 & 2.6802 & 0.3331 \\ 
   \hline
% d & 0.0033 & 0.0333 & 0.3331\\
 %\hline 
 %\caption{Spring stiffness co-efficient and damping co-efficient calculated for $M^*=1$ and different $S_{g}$ values.}
 %\label{table:2.1}
  \end{tabular}
 \caption{Spring stiffness co-efficient and damping co-efficient calculated for $M^*=1$ and different $S_{g}$ values.}
 \label{table:2.1}
 \end{centering}
 \end{table}
 %\end{center}
\section{Artificial Neural Network}
The simplest definition of a neural network, more properly referred to as an 'artificial' neural network (ANN), is provided by the inventor of one of the first neuro-computers, Dr. Robert Hecht-Nielsen.\\
\textit{"A computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs"},reference \citet{caudill1987neural}\\
An ANN is built upon the principle of a biological \textit{neuron}, which has \textit{dendrites} to receive signals, a \textit{cell body} to process them and an \textit{axon} to communicate the signals to the other neurons. Similarly ANNs are made up of layers, it can have a number of \textit{input layers}, a processing layer mostly referred to as \textit{hidden layers} and an \textit{output layer}. Figure \ref{fig:2.10} expresses the similarity between the biological neuron and an ANN.\\
\begin{figure}[h]
 \centering
 \includegraphics[height=3in]{ANN}
 \caption{Representation of a biological Neuron and an Artificial Neural Network}
 \label{fig:2.10}
\end{figure}
Neural networks are typically organized in layers. Layers are made up of a number of interconnected neurons which contain an \textit{activation function}. Patterns are presented to the network via the \textit{input layer}, which communicates to one or more \textit{hidden layers} where the actual processing is done by a system of \textit{weighted connections}. The hidden layers then link the result to an \textit{output layer}. Each neuron element has a numeric threshold value. The neuron compares the effective weight to this threshold value. If the effective weight exceeds the threshold, the unit produces an output value of 1. If it does not exceed the threshold it produces an output of 0. The process of adjusting the weights an threshold values in a neural net is called \textbf{training}. A neural net is trained with significant data-set to produce the desired results. Figure \ref{fig:2.11} represents an example of weights and biases and an activation function.\\
\begin{figure}[h]
 \centering
 \includegraphics[height=3in]{weights}
 \caption{Representation of weights, biases and an activation function which constitutes the working of an Artificial Neural Network.}
 \label{fig:2.11}
\end{figure}
The Artificial Neural Networks are very versatile and can be used for numerous applications. Based on their application, they are classified into various categories. According to \citet{jain1996artificial},  most applications of neural networks fall into the following categories:\\
\begin{itemize}
\item Pattern Classification: The task of pattern classification is to assign an input (speech or image) to pre-described classes. Classic examples are character recognition, speech recognition etc. 
\item Clustering or Categorization: This particular type comes under \textit{unsupervised learning}, where there are no training data with classified labels. The network explores the similarity between the patterns and places similar patterns into a cluster. Examples include data mining, data compression etc.
\item Prediction or Forecasting: From a set of time dependent samples, the network is trained to predict or forecast a sample at a future time frame. \textit{Stock market} and \textit{weather forecasting} are examples of this type.
\item Optimization: This is one of the most widely used technique in variety of fields such as mathematics, science, statistics, medicine and economics. The goal of an optimization algorithm is to minimize or maximize an objective function.
\end{itemize}
\subsection{Learning of an Artificial Neural Network}
The learning of an ANN can be attributed as the problem of updating network architecture and connection weights	in order that the network can predict or perform a specified task. The network usually must learn the connection weights from available training patterns. Performance is improved over time by iteratively updating the weights in the network. There are primarily three types of learning:
\begin{itemize}
\item Supervised learning: The network is provided with the output for every input pattern and the weights are adjusted iteratively in order to achieve the desired output.
\item Reinforcement learning: Reinforcement learning is a variant of supervised learning in which the network is provided with only a critique
on the correctness of network outputs, not the correct answers themselves.
\item Unsupervised learning: Unsupervised learning, does not require an exact output or target associated with each input pattern in the training data set. It explores the underlying structure in the data, or correlations between patterns in the data, and organizes patterns into categories from these correlations. Hybrid learning combines supervised and unsupervised learning. Part of the weights are usually determined through supervised learning, while the others are obtained through unsupervised learning.
\end{itemize}
In this study, the focus is on \textbf{Supervised learning}, where the deep neural network architecture is provided with inputs and associated targets.
\subsubsection{Cost function}
The neural network can be trained using batch gradient descent on a set of fixed training data $\{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \}$. The cost-function of a single training data can be expressed as in equation \ref{eqn:2.21}.\\
 \begin{equation}\label{eqn:2.21}
 J(W,b; x,y) = \frac{1}{2} \left\| h_{W,b}(x) - y \right\|^2
 \end{equation}
The above equation represents a squared-error cost function $J(W,b; x,y)$, which is dependent on weights and biases of a single training data. For the set of $m$ examples, the overall cost-function is defined to be:\\
 \begin{align}
 J(W,b)
 &= \left[ \frac{1}{m} \sum_{i=1}^m J(W,b;x^{(i)},y^{(i)}) \right]
                       + \frac{\lambda}{2} \sum_{l=1}^{n_l-1} \; \sum_{i=1}^{s_l} \; \sum_{j=1}^{s_{l+1}} \left( W^{(l)}_{ji} \right)^2
 \\
&= \left[ \frac{1}{m} \sum_{i=1}^m \left( \frac{1}{2} \left\| h_{W,b}(x^{(i)}) - y^{(i)} \right\|^2 \right) \right]
                       + \frac{\lambda}{2} \sum_{l=1}^{n_l-1} \; \sum_{i=1}^{s_l} \; \sum_{j=1}^{s_{l+1}} \left( W^{(l)}_{ji} \right)^2
 \end{align}
The first term in the above equation is an average sum-of-squares error term. The second term in the equation is a regularization term $\left(\lambda\right)$ also called as \textit{weight decay term}, which tends to decrease the magnitude of weights and helps prevent over-fitting. The primarily goal of learning of an ANN is to minimize the cost function $J(W,b)$ as a function of weights and biases. The \textbf{Back-propagation algorithm} proposed by \citet{rumelhart1986learning} is used to train the neural network model.
\subsubsection{Back-propagation algorithm}
In order to train a Deep Neural Network (An ANN with one or more hidden layers), the weights $W_{ij}^l$ connecting the nodes in layer $i$ to layer $j$ and corresponding biases $b_{i}^l$ needs to be randomly initialized to some value distributed around zero. Then an optimization algorithm such as \textit{steepest-descent method} or \textit{Marquardt levenberg algorithm} is applied. The \textit{back-propagation algorithm} can be applied only on \textit{Feed-Forward} or \textit{Multi Layer Perceptrons network}, and are ideal for Pattern Recognition and Regression models. The weights and biases of the network is updated backwards after an initial "forward pass". The algorithm is as described below:
\begin{itemize}
\item For a given training example $(x,y)$, a \textit{forward pass} to compute the activation throughout the network, including the output value of the hypothesis $h_{W,b}(x)$ is carried out. The activation for hidden layers $(L2,L3,\dots,L_{nl-1}$ and the output layer $L_{nl}$ is done during the forward pass. Here, $nl$ represents the output layer.
\item For each node $i$ in layer $l$, an error term $\delta_{i}^{l}$ that quantifies the impact of a particular node on the errors in the output is defined. For an output node, the difference between the network's activation and the target value can be used to define $\delta_{i}^{nl}$.
\begin{equation}
\delta^{(n_l)}_i
= \frac{\partial}{\partial z^{(n_l)}_i} \;\;
        \frac{1}{2} \left\|y - h_{W,b}(x)\right\|^2 = - (y_i - a^{(n_l)}_i) \cdot f'(z^{(n_l)}_i)
\end{equation}
\item For the hidden layers, the computation of error term $\delta_{i}^{l}$ is based on a weighted average of the error terms of the nodes for a corresponding input node $a_i^{\left(nl\right)}$. For the hidden layers $l = n_l-1, n_l-2, n_l-3, \ldots, 2$ and for each node $i$ in layer $l$, set:
\begin{equation}
\delta^{(l)}_i = \left( \sum_{j=1}^{s_{l+1}} W^{(l)}_{ji} \delta^{(l+1)}_j \right) f'(z^{(l)}_i)
\end{equation}
\item After computing the error terms of all corresponding layers, the partial derivatives of the cost function is given by:
\begin{align}
\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y) &= a^{(l)}_j \delta_i^{(l+1)} \\
\frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x, y) &= \delta_i^{(l+1)}.
\end{align}
\end{itemize}

After the back-propagation algorithm, based on the optimizer the complete algorithm of update rule differs. The algorithm mentioned below is considering \textit{Stochastic Gradient Method} as an optimizer. One iteration of the batch gradient descent is as follows:
\begin{itemize}
\item Considering $\Delta W^{(l)}$ and $\Delta b^{(l)}$ in the same dimensions of weights and biases, set: $\Delta W^{(l)} := 0$ and $\Delta b^{(l)} := 0$ for all layers $l$. 
\item For $i=1$ to $m$,
  \begin{enumerate}[a)]
   \item Back-propagation is done to obtain $\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y)$ and $\frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x, y)$.
   \item Set $\Delta W^{(l)} := \Delta W^{(l)}+\frac{\partial}{\partial W_{ij}^{(l)}} J(W,b; x, y)$
   \item Set $\Delta b^{(l)} := \Delta b^{(l)}+\frac{\partial}{\partial b_{i}^{(l)}} J(W,b; x, y)$
  \end{enumerate}
\item The weights and biases are updated:
\begin{align}
W^{(l)} &= W^{(l)} - \alpha \left[ \left(\frac{1}{m} \Delta W^{(l)} \right) + \lambda W^{(l)}\right] \\
b^{(l)} &= b^{(l)} - \alpha \left[\frac{1}{m} \Delta b^{(l)}\right]
\end{align}
where $\alpha$ is the \textit{learning rate} of the network.
\end{itemize}
\subsection{Factors influencing the learning of a network}
There are certain factors which influence the learning of a neural network. 
\begin{enumerate}[(i)]
\item Choice of optimizer
\item Activation function
\item Learning rate
\item Number of hidden layers
\end{enumerate}
\subsubsection{Choice of Optimizer}
There are many optimizer methods available to minimize the \textit{cost-function} such as \textit{Stochastic Gradient Descent}, \textit{Momentum}, \textit{{Adagrad}, {Adadelta}, {RMSprop},{Adam}} and Newton's optimization method \textit{L-BFGS} etc. The choice of optimizer method is purely application oriented, and in the current study the optimizer selected is \textbf{adam}.
\paragraph{Adam optimizer}
This method is an update to the \textit{stochastic gradient descent} approach which uses single learning rate for all weight updates, whereas in Adam the learning rate is adaptive. Adam is short for \textit{"adaptive moment estimation} and was proposed by \citet{DBLP:journals/corr/KingmaB14}. This method combines the advantages of two methods \textit{AdaGrad} and \textit{RMSProp}. The simplified form of the algorithm proposed by \citet{DBLP:journals/corr/KingmaB14} is as follows:
\begin{align}
m &= \beta_{1}m + \left(1-\beta_{1}\right)dx\\
v &= \beta_{2}v + \left(1-\beta_{2}\right)\left(dx^2\right)\\
x &= x - \frac{\alpha m}{\sqrt{v}+eps}
\end{align}
where $m$ and $v$ are momentum vectors, and $x$ is the position or point along the descent direction. As per the study of \citet{DBLP:journals/corr/KingmaB14}, the recommended values for exponential decay rates $\beta_1$ and $beta_2$ are $0.9$ and $0.999$ respectively. The effectiveness of this method can be observed from figure \ref{fig:2.13} taken from their study.
\begin{figure}[h]
\centering
\includegraphics[height=3in]{adam}
\caption{Performance of various optimizer on MNIST images data-set, taken from \citet{DBLP:journals/corr/KingmaB14}}
\label{fig:2.13}
\end{figure}
\subsubsection{Activation Function}
The activation function determines the output value of each neuron, it determines whether a particular neuron is activated or not. \citet{duch1999survey} had proposed over 640 different activation functions. There are some common activation functions available such as \textit{Perceptron activation}, \textit{sigmoid function}, \textit{Rectified Linear Units (ReLU)} and \textit{hyperbolic tan function}. The purpose of an activation function in a deep neural network is to ensure that the representation in the input space is mapped to a different space in the output. A similarity function between the inputs and weights are performed by the neural network. In the current study \textbf{ReLU} activation function has been selected. 
\paragraph{Rectified Linear Unit (ReLU) activation function:}
This is the most widely used activation function, across various applications. The function $\sigma(x) = max(0,x)$ returns only positive values for any given input $x$. This function does not require any normalization as the values returned are either $0$ or positive.
\begin{figure}[h]
\centering
\includegraphics[height=3in]{ReLU}
\caption{Representation of ReLU function output values for different input values}
\label{fig:2.13}
\end{figure}    
Figure \ref{fig:2.13} represents the functional behavior of a ReLU activation function.
\subsubsection{Learning rate}    
The learning rate has a direct impact on the loss function per epoch and ultimately the overall loss function of the training session. Low learning rates minimizes the loss linearly and it takes longer for the loss to decay to an acceptable value. High learning rates will decay the loss faster, however the overall loss value might be higher than the acceptable limit. Figure \ref{fig:2.14} depicts the effect of learning rate on training loss.
\begin{figure}[h]
\centering
\includegraphics[height=3in]{learning_rate}
\caption{Effect of learning rates on training loss}
\label{fig:2.14}
\end{figure}
\subsubsection{Number of hidden layers}
A deep neural network consists of one or more hidden layer units, in addition to the input and output layer. The hidden layers are required to fit the neural network model, $i.e$ to map any function from one finite space to another. However, the number of hidden layers is mostly application or problem specific and in this study 2 hidden layers are selected. Further number of hidden layers hinders the effectiveness of Back-propagation algorithm as the numerical error from one layer to the next and so on.  

The parameters used to build and train the Neural Network model is presented in the table \ref{table:2.2}.

\begin{table}[H]
 \centering
	\begin{tabular}{|c|c|}
	 \hline
	 Network type & Multi Layer Perceptron model\\
	 \hline
	 Optimizer method & Adam optimizer\\
	 \hline
	 Activation function & ReLU\\
	 \hline
	 Learning rate & 0.001\\
	 \hline
	 Number of hidden layers & 2\\
	 \hline
	 Number of training epochs & 500\\
	 \hline
	\end{tabular}
\caption{Features and parameters used to train the Tensorflow Neural Network model}
\label{table:2.2}
\end{table}








